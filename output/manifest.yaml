metadata:
  title: "Llama Stack \u2014 llama-stack  documentation"
  base_url: https://llama-stack.readthedocs.io
  generated_at: '2025-05-10T18:38:37.635423'
  page_count: 50
root_urls:
- https://llama-stack.readthedocs.io/en/latest/
pages:
  https://llama-stack.readthedocs.io/en/latest/:
    title: "Llama Stack \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest.md
    parent_url: null
    children:
    - https://llama-stack.readthedocs.io/en/latest/references/evals_reference/index.html
    - https://llama-stack.readthedocs.io/en/latest/references/llama_cli_reference/download_models.html
    - https://llama-stack.readthedocs.io/en/latest/references/llama_stack_client_cli_reference.html
    - https://llama-stack.readthedocs.io/en/latest/references/llama_cli_reference/index.html
    - https://llama-stack.readthedocs.io/en/latest/references/python_sdk_reference/index.html
    - https://llama-stack.readthedocs.io/en/latest/references/api_reference/index.html
    - https://llama-stack.readthedocs.io/en/latest/references/index.html
    - https://llama-stack.readthedocs.io/en/latest/contributing/new_api_provider.html
    - https://llama-stack.readthedocs.io/en/latest/contributing/index.html
    - https://llama-stack.readthedocs.io/en/latest/playground/index.html
    - https://llama-stack.readthedocs.io/en/latest/building_applications/safety.html
    - https://llama-stack.readthedocs.io/en/latest/building_applications/telemetry.html
    - https://llama-stack.readthedocs.io/en/latest/building_applications/evals.html
    - https://llama-stack.readthedocs.io/en/latest/building_applications/tools.html
    - https://llama-stack.readthedocs.io/en/latest/building_applications/agent_execution_loop.html
    - https://llama-stack.readthedocs.io/en/latest/building_applications/agent.html
    - https://llama-stack.readthedocs.io/en/latest/building_applications/rag.html
    - https://llama-stack.readthedocs.io/en/latest/building_applications/index.html
    - https://llama-stack.readthedocs.io/en/latest/distributions/building_distro.html
    - https://llama-stack.readthedocs.io/en/latest/distributions/kubernetes_deployment.html
    - https://llama-stack.readthedocs.io/en/latest/distributions/list_of_distributions.html
    - https://llama-stack.readthedocs.io/en/latest/distributions/configuration.html
    - https://llama-stack.readthedocs.io/en/latest/distributions/importing_as_library.html
    - https://llama-stack.readthedocs.io/en/latest/distributions/index.html
    - https://llama-stack.readthedocs.io/en/latest/providers/vector_io/weaviate.html
    - https://llama-stack.readthedocs.io/en/latest/providers/vector_io/milvus.html
    - https://llama-stack.readthedocs.io/en/latest/providers/vector_io/qdrant.html
    - https://llama-stack.readthedocs.io/en/latest/providers/vector_io/pgvector.html
    - https://llama-stack.readthedocs.io/en/latest/providers/vector_io/chromadb.html
    - https://llama-stack.readthedocs.io/en/latest/providers/vector_io/sqlite-vec.html
    - https://llama-stack.readthedocs.io/en/latest/providers/vector_io/faiss.html
    - https://llama-stack.readthedocs.io/en/latest/providers/external.html
    - https://llama-stack.readthedocs.io/en/latest/providers/index.html
    - https://llama-stack.readthedocs.io/en/latest/concepts/evaluation_concepts.html
    - https://llama-stack.readthedocs.io/en/latest/concepts/index.html
    - https://llama-stack.readthedocs.io/en/latest/introduction/index.html
    - https://llama-stack.readthedocs.io/en/latest/getting_started/detailed_tutorial.html
    - https://llama-stack.readthedocs.io/en/latest/getting_started/index.html
    summary: Llama Stack is an open-source framework for building generative AI applications,
      offering a unified API layer, a plugin architecture, prepackaged distributions,
      and SDKs for various languages. It supports various inference providers and
      aims to streamline the entire AI application development lifecycle.
  https://llama-stack.readthedocs.io/en/latest/references/evals_reference/index.html:
    title: "Evaluations \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-references-evals-reference-index-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children:
    - https://llama-stack.readthedocs.io/en/latest/index.html
    summary: This documentation describes the Llama Stack's evaluation flow, detailing
      APIs for dataset input/output, scoring, and evaluation execution, along with
      a Colab notebook for examples and a walkthrough for evaluating a model on open
      benchmarks like MMMU.
  https://llama-stack.readthedocs.io/en/latest/references/llama_cli_reference/download_models.html:
    title: "Downloading Models \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-references-llama-cli-reference-download-models-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
  https://llama-stack.readthedocs.io/en/latest/references/llama_stack_client_cli_reference.html:
    title: "llama (client-side) CLI Reference \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-references-llama-stack-client-cli-reference-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This documentation provides a CLI reference for interacting with a Llama
      Stack distribution, including commands for configuration, managing datasets,
      models, providers, and other functionalities. It shows example usage with `llama-stack-client`
      commands for listing providers and models, and getting model details.
  https://llama-stack.readthedocs.io/en/latest/references/llama_cli_reference/index.html:
    title: "llama (server-side) CLI Reference \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-references-llama-cli-reference-index-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This page documents the `llama` CLI tool for setting up and using the
      Llama Stack, including installation instructions and subcommands for downloading
      models, listing models, and building/running a Llama Stack server.
  https://llama-stack.readthedocs.io/en/latest/references/python_sdk_reference/index.html:
    title: "Python SDK Reference \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-references-python-sdk-reference-index-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This Python SDK reference documents types and methods for interacting
      with toolgroups, tools, and tool runtime, including methods for listing, getting,
      registering, unregistering, and invoking tools.
  https://llama-stack.readthedocs.io/en/latest/references/api_reference/index.html:
    title: "API Reference \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-references-api-reference-index-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This page provides a reference to the API, including an OpenAPI specification.
  https://llama-stack.readthedocs.io/en/latest/references/index.html:
    title: "References \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-references-index-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This page provides links to the API, Python SDK, and CLI references for
      the Llama Stack, which are useful resources for developers building and interacting
      with Llama Stack servers.
  https://llama-stack.readthedocs.io/en/latest/contributing/new_api_provider.html:
    title: "Adding a New API Provider \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-contributing-new-api-provider-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This guide explains how to add new API providers to Llama Stack, including
      determining provider type, adding it to the registry, updating templates, and
      testing the implementation. It also includes information about how to submit
      a PR.
  https://llama-stack.readthedocs.io/en/latest/contributing/index.html:
    title: "Contributing to Llama-Stack \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-contributing-index-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This page details the contribution process for the Llama-Stack project,
      including how to submit issues, pull requests, set up the development environment,
      and use pre-commit hooks.
  https://llama-stack.readthedocs.io/en/latest/index.html:
    title: "Llama Stack \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-index-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/references/evals_reference/index.html
    children: []
  https://llama-stack.readthedocs.io/en/latest/playground/index.html:
    title: "Llama Stack Playground \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-playground-index-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This documentation describes the Llama Stack Playground, an experimental
      interactive interface for exploring Llama Stack API capabilities and features
      like chatbots, evaluations, API inspection, and showcases how to start the playground.
  https://llama-stack.readthedocs.io/en/latest/building_applications/safety.html:
    title: "Safety Guardrails \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-building-applications-safety-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This page describes how to use Llama Stack's Shield system to incorporate
      safety guardrails into your AI application by registering a safety shield and
      running content through it to detect violations.
  https://llama-stack.readthedocs.io/en/latest/building_applications/telemetry.html:
    title: "Telemetry \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-building-applications-telemetry-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This documentation details the Llama Stack's telemetry system, covering
      events (logs, metrics, structured logs), spans, traces, sinks (OpenTelemetry,
      SQLite, Console), configuration, and visualization with Jaeger/querying SQLite.
  https://llama-stack.readthedocs.io/en/latest/building_applications/evals.html:
    title: "Evaluations \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-building-applications-evals-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This page documents how to evaluate LLM applications built with Llama
      Stack, covering topics from agent building and querying to response evaluation
      using the `/scoring` API. It provides code examples for building a search agent,
      querying execution steps, and evaluating responses.
  https://llama-stack.readthedocs.io/en/latest/building_applications/tools.html:
    title: "Tools \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-building-applications-tools-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This documentation describes tools in Llama Stack, which are functions
      that agents can use, organized into tool groups from different providers (built-in,
      MCP, and client-provided) like web search, WolframAlpha, and RAG. It includes
      code examples for registering and using these tools.
  https://llama-stack.readthedocs.io/en/latest/building_applications/agent_execution_loop.html:
    title: "Agent Execution Loop \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-building-applications-agent-execution-loop-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: The Llama Stack agent execution loop documentation describes the key
      steps involved in the agent's workflow, including safety checks, context retrieval,
      inference, tool usage, and monitoring configurations, using a sequence diagram
      and example code.
  https://llama-stack.readthedocs.io/en/latest/building_applications/agent.html:
    title: "Agents \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-building-applications-agent-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This page describes the core concepts of agents in Llama Stack, including
      agent configuration, sessions, turns, and steps, and provides example code snippets.
  https://llama-stack.readthedocs.io/en/latest/building_applications/rag.html:
    title: "Retrieval Augmented Generation (RAG) \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-building-applications-rag-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This documentation page provides an overview of Retrieval Augmented Generation
      (RAG) within the Llama Stack, including setting up vector databases, ingesting
      documents, retrieval methods, and using the RAG Tool within agents.
  https://llama-stack.readthedocs.io/en/latest/building_applications/index.html:
    title: "Building AI Applications (Examples) \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-building-applications-index-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This page provides links to example notebooks and documentation about
      building AI applications with Llama Stack, covering topics like RAG, agents,
      tools, evaluations, telemetry, and safety.
  https://llama-stack.readthedocs.io/en/latest/distributions/building_distro.html:
    title: "Build your own Distribution \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-distributions-building-distro-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This documentation outlines the steps for building a Llama Stack distribution,
      covering log level settings, building the stack from the repository, and using
      templates for different API providers via the command-line interface.
  https://llama-stack.readthedocs.io/en/latest/distributions/kubernetes_deployment.html:
    title: "Kubernetes Deployment Guide \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-distributions-kubernetes-deployment-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children:
    - https://llama-stack.readthedocs.io/en/latest/distributions/ondevice_distro/android_sdk.html
    summary: This documentation provides a guide on deploying the Llama Stack and
      vLLM servers in a Kubernetes cluster using Kind, including setting up a persistent
      volume claim, secrets, and deploying the vLLM server as a deployment and service.
  https://llama-stack.readthedocs.io/en/latest/distributions/list_of_distributions.html:
    title: "Available List of Distributions \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-distributions-list-of-distributions-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children:
    - https://llama-stack.readthedocs.io/en/latest/distributions/ondevice_distro/ios_sdk.html
    - https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/fireworks.html
    - https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/together.html
    - https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/ollama.html
    - https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/nvidia.html
    - https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/tgi.html
    - https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/meta-reference-gpu.html
    - https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/remote-vllm.html
    - https://llama-stack.readthedocs.io/en/latest/distributions/remote_hosted_distro/index.html
    summary: This page lists available Llama Stack distributions, categorized by use
      case like hosted endpoints, GPU hardware, local desktop, remote inference providers,
      and iOS/Android devices, with links to further documentation. The document provides
      links to Docker Hub and user guides for setup.
  https://llama-stack.readthedocs.io/en/latest/distributions/configuration.html:
    title: "Configuring a \u201CStack\u201D \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-distributions-configuration-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This documentation describes how to configure a Llama Stack runtime using
      a YAML file, including sections for APIs, providers (e.g., inference, vector_io,
      safety), resources, and models, along with example configurations.
  https://llama-stack.readthedocs.io/en/latest/distributions/importing_as_library.html:
    title: "Using Llama Stack as a Library \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-distributions-importing-as-library-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This page explains how to use Llama Stack as a Python library, bypassing
      server setup for external inference services, with setup instructions and example
      code for initializing the client and accessing its APIs.
  https://llama-stack.readthedocs.io/en/latest/distributions/index.html:
    title: "Distributions Overview \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-distributions-index-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This documentation page overviews pre-packaged Llama Stack distributions,
      covering topics like using Llama Stack as a library, configuring a stack, available
      distributions, Kubernetes deployment, and building custom distributions.
  https://llama-stack.readthedocs.io/en/latest/providers/vector_io/weaviate.html:
    title: "Weaviate \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-providers-vector-io-weaviate-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This page describes Weaviate, a vector database provider for Llama Stack,
      outlining its features like vector and full-text search, and directs users to
      the Weaviate documentation for installation and further details.
  https://llama-stack.readthedocs.io/en/latest/providers/vector_io/milvus.html:
    title: "Milvus \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-providers-vector-io-milvus-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This documentation provides instructions for configuring and using Milvus,
      a vector database, within the Llama Stack, including inline (local) and remote
      configurations with different TLS options. It details installation and configuration
      steps with code snippets and key parameters.
  https://llama-stack.readthedocs.io/en/latest/providers/vector_io/qdrant.html:
    title: "Qdrant \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-providers-vector-io-qdrant-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: Qdrant is an in-memory and remote vector database provider for Llama
      Stack, offering fast vector retrieval with features like metadata filtering
      and hybrid search is installed using Docker.
  https://llama-stack.readthedocs.io/en/latest/providers/vector_io/pgvector.html:
    title: "Postgres PGVector \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-providers-vector-io-pgvector-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This page describes PGVector, a remote vector database provider for Llama
      Stack, including its features, usage, and installation instructions using Docker.
  https://llama-stack.readthedocs.io/en/latest/distributions/ondevice_distro/android_sdk.html:
    title: "Llama Stack Client Kotlin API Library \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-distributions-ondevice-distro-android-sdk-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/distributions/kubernetes_deployment.html
    children: []
    summary: This documentation provides a guide on how to integrate the Llama Stack
      Kotlin library into an Android app, enabling both local and remote AI inferencing.
      It includes instructions for adding dependencies, setting up remote inferencing,
      and a link to a demo app.
  https://llama-stack.readthedocs.io/en/latest/distributions/ondevice_distro/ios_sdk.html:
    title: "iOS SDK \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-distributions-ondevice-distro-ios-sdk-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/distributions/list_of_distributions.html
    children: []
    summary: This documentation describes the iOS SDK for the Llama Stack, which allows
      developers to use Llama Stack for remote or on-device inference and provides
      installation instructions and code examples.
  https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/fireworks.html:
    title: "Fireworks Distribution \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-distributions-self-hosted-distro-fireworks-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/distributions/list_of_distributions.html
    children: []
    summary: This documentation provides information on the `llamastack/distribution-fireworks`,
      including its provider configurations, environment variables, available models,
      and instructions for running it using Docker or Conda.
  https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/together.html:
    title: "Together Distribution \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-distributions-self-hosted-distro-together-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/distributions/list_of_distributions.html
    children: []
    summary: This documentation page describes the configurations and environment
      variables for the `llamastack/distribution-together` distribution, including
      available models and instructions for running the distribution with Docker or
      Conda.
  https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/ollama.html:
    title: "Ollama Distribution \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-distributions-self-hosted-distro-ollama-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/distributions/list_of_distributions.html
    children: []
    summary: This documentation provides details on configuring and running the `llamastack/distribution-ollama`
      distribution, including setting up the Ollama server, environment variables,
      and running Llama Stack with Docker or Conda.
  https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/nvidia.html:
    title: "NVIDIA Distribution \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-distributions-self-hosted-distro-nvidia-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/distributions/list_of_distributions.html
    children: []
    summary: This document describes the NVIDIA distribution for the llamastack, including
      available APIs, environment variables, supported models, prerequisites, and
      supported services.
  https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/tgi.html:
    title: "TGI Distribution \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-distributions-self-hosted-distro-tgi-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/distributions/list_of_distributions.html
    children: []
    summary: This documentation describes the `llamastack/distribution-tgi`, detailing
      its provider configurations, environment variables, and setup instructions,
      including Docker examples for running TGI servers for inference and safety models.
  https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/meta-reference-gpu.html:
    title: "Meta Reference Distribution \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-distributions-self-hosted-distro-meta-reference-gpu-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/distributions/list_of_distributions.html
    children: []
    summary: This documentation describes the configuration of the `llamastack/distribution-meta-reference-gpu`
      distribution, including API providers, environment variables, and instructions
      for running the distribution using Docker or Conda.
  https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/remote-vllm.html:
    title: "Remote vLLM Distribution \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-distributions-self-hosted-distro-remote-vllm-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/distributions/list_of_distributions.html
    children: []
    summary: This documentation describes the `llamastack/distribution-remote-vllm`
      distribution, detailing its provider configurations, configurable environment
      variables, and setup instructions for running a vLLM server for inference, including
      examples using AMD GPUs.
  https://llama-stack.readthedocs.io/en/latest/distributions/remote_hosted_distro/index.html:
    title: "Remote-Hosted Distributions \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-distributions-remote-hosted-distro-index-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/distributions/list_of_distributions.html
    children: []
    summary: This page lists remote endpoints for Llama Stack API, detailing distributions
      like Together and Fireworks with their endpoints and links to client documentation.
  https://llama-stack.readthedocs.io/en/latest/providers/vector_io/chromadb.html:
    title: "Chroma \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-providers-vector-io-chromadb-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This page describes Chroma, an inline and remote vector database provider,
      and provides installation instructions for use in a Llama Stack project. It
      outlines Chroma's features, usage, and installation, directing users to further
      documentation.
  https://llama-stack.readthedocs.io/en/latest/providers/vector_io/sqlite-vec.html:
    title: "SQLite-Vec \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-providers-vector-io-sqlite-vec-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: SQLite-Vec is an inline vector database provider for Llama Stack that
      stores and queries vectors directly within an SQLite database, ideal for scenarios
      with frequent writes, large datasets, and disk-based persistence.  It includes
      installation instructions and a comparison to Faiss.
  https://llama-stack.readthedocs.io/en/latest/providers/vector_io/faiss.html:
    title: "Faiss \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-providers-vector-io-faiss-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: Faiss is an in-memory vector database provider for Llama Stack, offering
      fast vector retrieval.  Install it via pip and then configure it within your
      Llama Stack project to store and query vectors.
  https://llama-stack.readthedocs.io/en/latest/providers/external.html:
    title: "External Providers \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-providers-external-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This documentation describes how to configure and use external providers
      in Llama Stack, including specifying directories, provider types (remote and
      inline), and example configurations for custom providers, along with a list
      of known providers.
  https://llama-stack.readthedocs.io/en/latest/providers/index.html:
    title: "Providers Overview \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-providers-index-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: 'This page provides an overview of Llama Stack providers, which are different
      implementations for APIs like LLM inference, vector databases, and safety providers
      and describes the two flavors of providers: remote and inline. It also touches
      upon other components of llama stack such as Agents, DatasetIO, and Vector IO.'
  https://llama-stack.readthedocs.io/en/latest/concepts/evaluation_concepts.html:
    title: "Evaluation Concepts \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-concepts-evaluation-concepts-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This documentation describes the Llama Stack's evaluation flow, including
      APIs for dataset input/output, scoring, and evaluation, alongside supported
      open-benchmarks (MMLU-COT, GPQA-COT, SimpleQA, MMMU) and CLI commands for running
      evaluations.
  https://llama-stack.readthedocs.io/en/latest/concepts/index.html:
    title: "Core Concepts \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-concepts-index-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This page introduces the core concepts of Llama Stack, including its
      API-first design, API providers (remote and inline), resources (Model, Shield,
      etc.), and Distributions (pre-packaged versions).
  https://llama-stack.readthedocs.io/en/latest/introduction/index.html:
    title: "Why Llama Stack? \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-introduction-index-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: Llama Stack is a service-oriented, API-first framework designed to simplify
      the development and deployment of production-ready AI applications by addressing
      infrastructure complexity, providing essential capabilities like RAG and safety
      guardrails, and ensuring provider independence.
  https://llama-stack.readthedocs.io/en/latest/getting_started/detailed_tutorial.html:
    title: "Detailed Tutorial \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-getting-started-detailed-tutorial-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This documentation provides a step-by-step tutorial for setting up and
      running a local RAG agent using Llama Stack with Ollama, covering installation,
      environment setup, and server execution via different methods (venv, conda,
      and container).
  https://llama-stack.readthedocs.io/en/latest/getting_started/index.html:
    title: "Quickstart \u2014 llama-stack  documentation"
    markdown_path: llama-stack-readthedocs-io-en-latest-getting-started-index-html.md
    parent_url: https://llama-stack.readthedocs.io/en/latest/
    children: []
    summary: This quickstart guide provides step-by-step instructions for setting
      up and running a Retrieval-Augmented Generation (RAG) application locally using
      Llama Stack with Ollama as the inference provider.
